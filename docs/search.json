[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nYuzhi Tao\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYuzhi Tao\n\n\nMay 3, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuzhi Tao",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, more than 50,000 prior donors to a politically progressive nonprofit organization were randomly assigned to receive one of several fundraising letters via direct mail. The control group received a standard appeal for donations. The treatment group received one of several versions of a letter announcing a matching grant: either a 1:1, 2:1, or 3:1 match, meaning the donor’s contribution would be matched by $1, $2, or $3 for every $1 given, respectively. The letters also varied in the maximum size of the matching grant, which was either $25,000, $50,000, $100,000, or left unspecified.The letters also gave people a hint about how much to give, based on how much they gave before.\nThis design allows for analysis of how different aspects of a fundraising message affect donor behavior, specifically: - whether people donate at all - how much they give - whether larger matching ratios lead to higher engagement\nThe randomized design ensures that the causal effect of the message content can be estimated.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, more than 50,000 prior donors to a politically progressive nonprofit organization were randomly assigned to receive one of several fundraising letters via direct mail. The control group received a standard appeal for donations. The treatment group received one of several versions of a letter announcing a matching grant: either a 1:1, 2:1, or 3:1 match, meaning the donor’s contribution would be matched by $1, $2, or $3 for every $1 given, respectively. The letters also varied in the maximum size of the matching grant, which was either $25,000, $50,000, $100,000, or left unspecified.The letters also gave people a hint about how much to give, based on how much they gave before.\nThis design allows for analysis of how different aspects of a fundraising message affect donor behavior, specifically: - whether people donate at all - how much they give - whether larger matching ratios lead to higher engagement\nThe randomized design ensures that the causal effect of the message content can be estimated.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis dataset includes donation behavior and treatment assignments from a large field experiment on charitable giving. It contains a rich set of variables covering match offers, donation history, demographics, and political/geographic characteristics.\n\n\n\n\n\n\nDataset Table\n\n\n\n\n\n\nlibrary(haven)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\ndata\n\n# A tibble: 50,083 × 51\n   treatment control ratio    ratio2 ratio3 size    size25 size50 size100 sizeno\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl+lb&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl+l&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 2         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 3         1       0 1             0      0 3 [$10…      0      0       1      0\n 4         1       0 1             0      0 4 [Uns…      0      0       0      1\n 5         1       0 1             0      0 2 [$50…      0      1       0      0\n 6         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 7         1       0 1             0      0 1 [$25…      1      0       0      0\n 8         1       0 2             1      0 3 [$10…      0      0       1      0\n 9         1       0 2             1      0 4 [Uns…      0      0       0      1\n10         1       0 1             0      0 1 [$25…      1      0       0      0\n# ℹ 50,073 more rows\n# ℹ 41 more variables: ask &lt;dbl+lbl&gt;, askd1 &lt;dbl&gt;, askd2 &lt;dbl&gt;, askd3 &lt;dbl&gt;,\n#   ask1 &lt;dbl&gt;, ask2 &lt;dbl&gt;, ask3 &lt;dbl&gt;, amount &lt;dbl&gt;, gave &lt;dbl&gt;,\n#   amountchange &lt;dbl&gt;, hpa &lt;dbl&gt;, ltmedmra &lt;dbl&gt;, freq &lt;dbl&gt;, years &lt;dbl&gt;,\n#   year5 &lt;dbl&gt;, mrm2 &lt;dbl&gt;, dormant &lt;dbl&gt;, female &lt;dbl&gt;, couple &lt;dbl&gt;,\n#   state50one &lt;dbl&gt;, nonlit &lt;dbl&gt;, cases &lt;dbl&gt;, statecnt &lt;dbl&gt;,\n#   stateresponse &lt;dbl&gt;, stateresponset &lt;dbl&gt;, stateresponsec &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nBefore analyzing the effects of the treatment, it’s important to confirm that the treatment and control groups were similar to begin with. To do this, I test whether key baseline characteristics differ between the groups. This ensures that any observed outcomes can be confidently attributed to the treatment itself.\n\ntreated &lt;- filter(data, treatment == 1)\ncontrol &lt;- filter(data, treatment == 0)\n\nmean_treat &lt;- mean(treated$mrm2, na.rm = TRUE)\nmean_control &lt;- mean(control$mrm2, na.rm = TRUE)\n\nsd_treat &lt;- sd(treated$mrm2, na.rm = TRUE)\nsd_control &lt;- sd(control$mrm2, na.rm = TRUE)\n\nn_treat &lt;- sum(!is.na(treated$mrm2))\nn_control &lt;- sum(!is.na(control$mrm2))\n\nt_stat &lt;- (mean_treat - mean_control) / sqrt((sd_treat^2 / n_treat) + (sd_control^2 / n_control))\nt_stat\n\n[1] 0.1195316\n\nlm_balance &lt;- lm(mrm2 ~ treatment, data = data)\nsummary(lm_balance)\n\n\nCall:\nlm(formula = mrm2 ~ treatment, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.012  -9.012  -5.012   6.002 154.988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.99814    0.09353 138.979   &lt;2e-16 ***\ntreatment    0.01369    0.11453   0.119    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.08 on 50080 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  2.851e-07, Adjusted R-squared:  -1.968e-05 \nF-statistic: 0.01428 on 1 and 50080 DF,  p-value: 0.9049\n\n\nI compare the mrm2 between the treatment and control groups. Using both a manual t-test and a linear regression, I find no statistically significant difference. The t-statistic is 0.1195 and the regression p-value is 0.905. Both results show no statistically significant difference.\nThese findings match the results in Table 1 of the original paper, where the means for this variable are 13.012 for treatment group., and 12.998 for control group. Combined with similar standard deviations for both around 12.08, the small difference supports the idea that the groups are nearly identical on this pre-treatment characteristic.\nTable 1 is important because it shows that any later differences in giving behavior are likely due to the treatment, not pre-existing differences in the people themselves. That’s the power of random assignment, and allows this experiment to make strong causal claims.\n\n# female variable\n\nmean_female_treat &lt;- mean(treated$female, na.rm = TRUE)\nmean_female_control &lt;- mean(control$female, na.rm = TRUE)\n\nsd_female_treat &lt;- sd(treated$female, na.rm = TRUE)\nsd_female_control &lt;- sd(control$female, na.rm = TRUE)\n\nn_female_treat &lt;- sum(!is.na(treated$female))\nn_female_control &lt;- sum(!is.na(control$female))\n\nt_female &lt;- (mean_female_treat - mean_female_control) / \n  sqrt((sd_female_treat^2 / n_female_treat) + (sd_female_control^2 / n_female_control))\nt_female\n\n[1] -1.753513\n\nsummary(lm(female ~ treatment, data = data))\n\n\nCall:\nlm(formula = female ~ treatment, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2827 -0.2752 -0.2752  0.7173  0.7248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.282698   0.003504  80.688   &lt;2e-16 ***\ntreatment   -0.007547   0.004292  -1.758   0.0787 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4478 on 48970 degrees of freedom\n  (1111 observations deleted due to missingness)\nMultiple R-squared:  6.313e-05, Adjusted R-squared:  4.271e-05 \nF-statistic: 3.092 on 1 and 48970 DF,  p-value: 0.07869\n\n\nThen I tested the female variable. The t-statistic is -1.75 and the p-value from the regression is 0.079. This means we fail to reject the null hypothesis of no difference. In Table 1 of the paper, the proportion of females is 0.275 in treatment and 0.283 in control, so it is nearly identical to test results.\n\n# freq variable\n\nmean_freq_treat &lt;- mean(treated$freq, na.rm = TRUE)\nmean_freq_control &lt;- mean(control$freq, na.rm = TRUE)\n\nsd_freq_treat &lt;- sd(treated$freq, na.rm = TRUE)\nsd_freq_control &lt;- sd(control$freq, na.rm = TRUE)\n\nn_freq_treat &lt;- sum(!is.na(treated$freq))\nn_freq_control &lt;- sum(!is.na(control$freq))\n\nt_freq &lt;- (mean_freq_treat - mean_freq_control) / \n  sqrt((sd_freq_treat^2 / n_freq_treat) + (sd_freq_control^2 / n_freq_control))\nt_freq\n\n[1] -0.110845\n\nsummary(lm(freq ~ treatment, data = data))\n\n\nCall:\nlm(formula = freq ~ treatment, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -8.035  -6.047  -4.035   1.953 209.965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.04734    0.08821  91.231   &lt;2e-16 ***\ntreatment   -0.01198    0.10802  -0.111    0.912    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.39 on 50081 degrees of freedom\nMultiple R-squared:  2.455e-07, Adjusted R-squared:  -1.972e-05 \nF-statistic: 0.0123 on 1 and 50081 DF,  p-value: 0.9117\n\n\nI also tested freq, the number of prior donations, to assess whether this characteristic differed between treatment and control groups. The t-statistic is -0.11 and the regression p-value is 0.9117, indicating no statistically significant difference as well. These results match what we see in Table 1, where the average number of prior donations is 8.035 in treatment and 8.047 in control.\nThis further supports the idea that randomization was effective, and that any later differences in donation behavior are not due to differences in variables between the groups, just as shown in Table 1."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nTo begin exploring treatment effects, I compare the donation rate between the treatment and control groups. A simple barplot shows the proportion of people who donated in each group. This offers a quick visual sense of whether the matching offer may have had an impact.\n\ndonate_rate &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(rate = mean(gave, na.rm = TRUE)) %&gt;%\n  mutate(treatment = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\nggplot(donate_rate, aes(x = treatment, y = rate, fill = treatment)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(title = \"Proportion of People Who Donated\",\n       x = \"Group\",\n       y = \"Donation Rate\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1))\n\n\n\n\n\n\n\n\nTo formally test the difference in donation rates, I conduct both a t-test and a linear regression. These methods help quantify the effect of the match offer and determine if the observed difference is statistically significant. The goal is to confirm the patterns seen in the visual comparison.\n\n# t-test using formula\nmean_gave_treat &lt;- mean(treated$gave, na.rm = TRUE)\nmean_gave_control &lt;- mean(control$gave, na.rm = TRUE)\n\nsd_gave_treat &lt;- sd(treated$gave, na.rm = TRUE)\nsd_gave_control &lt;- sd(control$gave, na.rm = TRUE)\n\nn_gave_treat &lt;- sum(!is.na(treated$gave))\nn_gave_control &lt;- sum(!is.na(control$gave))\n\nt_stat_gave &lt;- (mean_gave_treat - mean_gave_control) / \n  sqrt((sd_gave_treat^2 / n_gave_treat) + (sd_gave_control^2 / n_gave_control))\nt_stat_gave\n\n[1] 3.209462\n\n# regression\nlm_gave &lt;- lm(gave ~ treatment, data = data)\nsummary(lm_gave)\n\n\nCall:\nlm(formula = gave ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\nAs we can see in the table, the average donation rate in the treatment group was about 2.2%, and 1.8% in the control group. This difference may sound small, but in the context of large-scale fundraising, it can mean a meaningful increase in revenue.\nThe t-test gave a t-statistic of 3.209462, and the linear regression returned a p-value of 0.0019. These results show a statistically significant increase in donation likelihood for those who received the matching offer.\nEven though the percentage difference is small, it matters in large-scale fundraising. These results support the paper’s main point that even small tweaks in message framing, like offering a match, can make a meaningful difference in behavior.\nAfter examining donation amounts among those who gave, I return to the broader question: does simply offering a match influence whether people donate at all? This is the central behavioral claim in the study, and it is where the strongest treatment effects are observed.\nTo answer this, I first compare donation rates between the treatment and control groups using a t-test and regression. I then estimate a probit model and calculate marginal effects to directly replicate the results shown in Table 3, Column 1 of the original paper.\n\nlibrary(margins)\n\nWarning: package 'margins' was built under R version 4.3.3\n\n# Probit model\nprobit_model &lt;- glm(gave ~ treatment, data = data, family = binomial(link = \"probit\"))\n\n# Marginal effects\nmargins_model &lt;- margins(probit_model)\nsummary(margins_model)\n\n    factor    AME     SE      z      p  lower  upper\n treatment 0.0043 0.0014 3.1044 0.0019 0.0016 0.0070\n\n\nThe estimated effect was 0.0043, meaning that a matching offer increases the likelihood of donating by about 0.43 percentage points, which is nearly identical to the 0.004 reported in Table 3 of the original study.\nThis confirms the earlier conclusion. While the increase in giving is small in absolute terms, it is statistically reliable and meaningful in a large-scale fundraising context. The matching offer acts as a subtle behavioral nudge, that encourage action from donors who may otherwise remain passive.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nNext, I examine whether larger match ratios lead to higher donation rates. I use a series of t-tests to compare 1:1, 2:1, and 3:1 match offers. This helps assess whether donors respond more strongly to larger matches.\n\n# Subset data to just treated people with a match offer\nmatch_data &lt;- data %&gt;% filter(treatment == 1)\n\n# Subset into 3 groups by match ratio\ngroup_1_1 &lt;- match_data %&gt;% filter(ratio == 1)\ngroup_2_1 &lt;- match_data %&gt;% filter(ratio == 2)\ngroup_3_1 &lt;- match_data %&gt;% filter(ratio == 3)\n\n# Mean donation rates\nmean(group_1_1$gave, na.rm = TRUE)\n\n[1] 0.02074912\n\nmean(group_2_1$gave, na.rm = TRUE)\n\n[1] 0.02263338\n\nmean(group_3_1$gave, na.rm = TRUE)\n\n[1] 0.0227334\n\n# T-test: 2:1 vs 1:1\nt.test(group_2_1$gave, group_1_1$gave)\n\n\n    Welch Two Sample t-test\n\ndata:  group_2_1$gave and group_1_1$gave\nt = 0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.001942773  0.005711275\nsample estimates:\n mean of x  mean of y \n0.02263338 0.02074912 \n\n# T-test: 3:1 vs 2:1\nt.test(group_3_1$gave, group_2_1$gave)\n\n\n    Welch Two Sample t-test\n\ndata:  group_3_1$gave and group_2_1$gave\nt = 0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.003811996  0.004012044\nsample estimates:\n mean of x  mean of y \n0.02273340 0.02263338 \n\n\nI tested whether larger match ratios (2:1 and 3:1) were more effective than the 1:1 match at encouraging donations. The average donation rates were slightly higher in the higher match groups, showing: - 1:1 → 2.07% - 2:1 → 2.26% - 3:1 → 2.27%\nHowever, the t-tests show that these small increases are not statistically significant: - 2:1 vs 1:1: p = 0.33 - 3:1 vs 2:1: p = 0.96\nThese results are consistent with the authors’ interpretation in the paper. While matching offers increase giving overall, larger match ratios do not provide an additional boost. So although donors respond positively to the idea of a match, they don’t seem to care much about the size of the match that any match is enough to increase motivation.\nTo further investigate this question, I run a regression with indicator variables for each match ratio. This allows me to compare the effectiveness of different match sizes within a unified framework. The omitted category serves as a baseline for interpreting relative differences.\n\n# Create ratio1 dummy: 1 if ratio == 1, else 0\ndata$ratio1 &lt;- ifelse(data$ratio == 1, 1, 0)\n\n# Only include treated group (people who received any match offer)\nmatch_only &lt;- filter(data, treatment == 1)\n\n# Run regression on 3 dummy variables: ratio1, ratio2, ratio3\nlm_ratio &lt;- lm(gave ~ ratio1 + ratio2 + ratio3, data = match_only)\nsummary(lm_ratio)\n\n\nCall:\nlm(formula = gave ~ ratio1 + ratio2 + ratio3, data = match_only)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.022733   0.001392  16.335   &lt;2e-16 ***\nratio1      -0.001984   0.001968  -1.008    0.313    \nratio2      -0.000100   0.001968  -0.051    0.959    \nratio3             NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\nIn the regression output, the coefficient for ratio1 (1:1 match) is -0.002 and not statistically significant (p = 0.313), and the coefficient for ratio2 (2:1 match) is even smaller at -0.0001, with a p-value of 0.959. The variable ratio3 (3:1) was dropped due to perfect multicollinearity, which is expected since only two indicators are needed to define a three-level categorical variable.\nThe overall regression has an extremely low R-squared and a p-value of 0.52, confirming that differences in match size do not meaningfully explain variation in donation behavior. These results reinforce the key takeaway in the origional paper, that the presence of a match offer matters more than how large the match is.\nI then compare the response rate differences between match types both directly from the data and using fitted regression coefficients. This dual approach helps confirm the consistency of the findings. It also provides insight into whether larger match ratios meaningfully shift donor behavior.\n\nmean_1_1 &lt;- mean(group_1_1$gave, na.rm = TRUE)\nmean_2_1 &lt;- mean(group_2_1$gave, na.rm = TRUE)\nmean_3_1 &lt;- mean(group_3_1$gave, na.rm = TRUE)\n\ndiff_2_1_vs_1_1 &lt;- mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 &lt;- mean_3_1 - mean_2_1\n\ndiff_2_1_vs_1_1\n\n[1] 0.001884251\n\ndiff_3_1_vs_2_1\n\n[1] 0.000100024\n\n\nThe differences in donation response rates between match ratios were assessed using both raw data and regression estimates. From the raw data, the difference between the 2:1 and 1:1 match ratios was about 0.19%, and between 3:1 and 2:1 was just 0.01%. Using the fitted coefficients from the regression, I got identical results that the size of the match ratio has very little impact on the likelihood of giving.\nThese findings reinforce the main takeaway from the earlier analysis and the original paper. While offering a match increases donations, increasing the match ratio beyond 1:1 does not lead to meaningful gains. Donors appear to be influenced by the presence of a match itself but not its size.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nBeyond participation rates, I also evaluate whether the match offer affects the amount donated. This analysis includes all individuals, even those who gave $0. A t-test and regression help determine if the match offer increased average donation size.\n\n# Compare average donation amount (includes 0s for non-donors)\nmean_amount_treat &lt;- mean(treated$amount, na.rm = TRUE)\nmean_amount_control &lt;- mean(control$amount, na.rm = TRUE)\nmean_amount_treat\n\n[1] 0.9668733\n\nmean_amount_control\n\n[1] 0.8132678\n\n# T-test\nt.test(treated$amount, control$amount)\n\n\n    Welch Two Sample t-test\n\ndata:  treated$amount and control$amount\nt = 1.9183, df = 36216, p-value = 0.05509\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.003344493  0.310555423\nsample estimates:\nmean of x mean of y \n0.9668733 0.8132678 \n\n# Regression: amount ~ treatment\nlm_amount &lt;- lm(amount ~ treatment, data = data)\nsummary(lm_amount)\n\n\nCall:\nlm(formula = amount ~ treatment, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.97  -0.97  -0.97  -0.81 399.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.81327    0.06742  12.063   &lt;2e-16 ***\ntreatment    0.15361    0.08256   1.861   0.0628 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.709 on 50081 degrees of freedom\nMultiple R-squared:  6.911e-05, Adjusted R-squared:  4.915e-05 \nF-statistic: 3.461 on 1 and 50081 DF,  p-value: 0.06282\n\n\nThe average donation amount was $0.97 in the treatment group and $0.81 in the control group, which results a modest difference. Statistical testing indicates that this difference is marginally significant, with p-values slightly above the conventional 5% threshold that 0.055 for the t-test and 0.063 for the regression.\nThese findings suggest that matching donation offers may not only encourage more people to give but also lead to slightly higher donation amounts on average. Although the effect is not strong in statistical terms, it is directionally consistent with the hypothesis that perceived impact increases generosity. In large-scale fundraising, even small increases in average gift size can contribute meaningfully to overall campaign success.\nAfter examining the effect of the matching offer on donation amounts across the full sample, the next step is to focus only on individuals who actually donated. This conditional analysis helps answer a more specific question: Does the treatment affect how much people give, once they’ve already decided to donate?\n\n# Filter to only include donors (positive donation amount)\ndonors_only &lt;- filter(data, gave == 1)\n\n# Regression: amount ~ treatment, among donors only\nlm_donor_amount &lt;- lm(amount ~ treatment, data = donors_only)\nsummary(lm_donor_amount)\n\n\nCall:\nlm(formula = amount ~ treatment, data = donors_only)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43.54 -23.87 -18.87   6.13 356.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.540      2.423  18.792   &lt;2e-16 ***\ntreatment     -1.668      2.872  -0.581    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 1032 degrees of freedom\nMultiple R-squared:  0.0003268, Adjusted R-squared:  -0.0006419 \nF-statistic: 0.3374 on 1 and 1032 DF,  p-value: 0.5615\n\n\nAmong donors, the average contribution was approximately $45.54, with no meaningful difference between treatment and control groups. The estimated effect of the matching offer was about –$1.67, and this difference is not statistically significant with a p-value of 0.56. This implies that the match offer did not influence the amount given giving the condition on donating.\nBecause treatment was randomly assigned, the regression coefficient still has a causal interpretation. However, the evidence here suggests that matching offers primarily affect the likelihood of giving, rather than how much is given once that decision is made.\nTo better understand how donation behavior differs between groups, I visualize the distribution of donation amounts among donors only. The red dashed line in each histogram represents the average donation for that group.\n\ndonors &lt;- filter(data, gave == 1)\n\nmean_treat &lt;- mean(donors$amount[donors$treatment == 1], na.rm = TRUE)\nmean_control &lt;- mean(donors$amount[donors$treatment == 0], na.rm = TRUE)\n\n# Plot for treatment group\nggplot(filter(donors, treatment == 1), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"lightblue\", color = \"white\") +\n  geom_vline(xintercept = mean_treat, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Treatment Group: Donation Amounts\",\n       x = \"Donation Amount\",\n       y = \"Number of Donors\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n# Plot for control group\nggplot(filter(donors, treatment == 0), aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"lightgray\", color = \"white\") +\n  geom_vline(xintercept = mean_control, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Control Group: Donation Amounts\",\n       x = \"Donation Amount\",\n       y = \"Number of Donors\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nVisually, both groups display a similar pattern: most donations fall below $100, with a long right tail of higher-value contributions. The treatment group shows a slightly higher concentration around the mean, but the overall shape of the distribution is nearly identical.\nThe average donation in the treatment group appears modestly higher, but this visual evidence supports the regression results: once someone decides to give, the presence of a matching offer does not lead them to donate significantly more. This reinforces the earlier takeaway that the matching incentive affects the decision to give, rather than the amount given."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo help build intuition for the reliability of statistical estimates, I use a simulation to demonstrate the Law of Large Numbers (LLN). This exercise mimics running many repeated experiments and shows how, as the number of simulations increases, the average difference in donation rates between the treatment and control groups stabilizes near the true underlying value.\n\nset.seed(42)\n\n# Simulate 10,000 draws from control and treatment (Bernoulli trials)\ncontrol_sim &lt;- rbinom(10000, 1, 0.018)\ntreat_sim &lt;- rbinom(10000, 1, 0.022)\n\n# Difference at each iteration\ndiffs &lt;- treat_sim - control_sim\n\n# Cumulative average\ncum_avg &lt;- cumsum(diffs) / seq_along(diffs)\n\n# True difference in means\ntrue_diff &lt;- 0.022 - 0.018\n\n# Plot\nplot(cum_avg, type = \"l\", col = \"steelblue\", lwd = 2,\n     main = \"Law of Large Numbers: Cumulative Difference in Means\",\n     xlab = \"Number of Simulations\", ylab = \"Cumulative Average Difference\")\nabline(h = true_diff, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topright\", legend = c(\"Cumulative Avg\", \"True Difference\"),\n       col = c(\"steelblue\", \"red\"), lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\nThe plot above shows the cumulative average difference in donation rates between simulated treatment and control groups over 10,000 draws. Early in the simulation, the average fluctuates quite a bit that sometimes even swinging in the wrong direction. This is because it’s based on just a few observations. But as more simulated observations accumulate, the average stabilizes and gradually converges to the true difference of 0.004, shown as the red dashed line.\nThis is a direct illustration of the Law of Large Numbers: as sample size increases, the sample average becomes a more reliable estimate of the population parameter. In this context, it reassures us that with large enough samples, like in the real experiment, even small differences in giving behavior can be detected and trusted.\n\n\nCentral Limit Theorem\nWhile the Law of Large Numbers shows that averages stabilize with more data, the Central Limit Theorem (CLT) explains something equally important: when we take averages from many repeated samples, the distribution of those averages becomes approximately normal, even if the original data is not. The following simulations demonstrate this by repeatedly sampling average differences in donation rates between treatment and control groups at different sample sizes.\n\nset.seed(123)\n\n# Function to simulate sampling distribution of differences\nsimulate_diff_means &lt;- function(n, reps = 1000, p_control = 0.018, p_treatment = 0.022) {\n  replicate(reps, {\n    control &lt;- rbinom(n, 1, p_control)\n    treatment &lt;- rbinom(n, 1, p_treatment)\n    mean(treatment) - mean(control)\n  })\n}\n\nsizes &lt;- c(50, 200, 500, 1000)\n\n# Run simulation for each sample size\ndiffs_50 &lt;- simulate_diff_means(50)\ndiffs_200 &lt;- simulate_diff_means(200)\ndiffs_500 &lt;- simulate_diff_means(500)\ndiffs_1000 &lt;- simulate_diff_means(1000)\n\nplot_hist &lt;- function(diffs, n) {\n  hist(diffs, breaks = 30, col = \"lightblue\", border = \"white\",\n       main = paste(\"Sample Size =\", n),\n       xlab = \"Average Difference in Donation Rates\")\n  abline(v = mean(diffs), col = \"red\", lwd = 2, lty = 2)\n}\n\npar(mfrow = c(2, 2))  \nplot_hist(diffs_50, 50)\nplot_hist(diffs_200, 200)\nplot_hist(diffs_500, 500)\nplot_hist(diffs_1000, 1000)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1)) \n\nEach panel in the figure above shows the distribution of average differences in donation rates between treatment and control groups across 1,000 simulated experiments. As the sample size increases from 50 to 1,000, the distribution becomes tighter and more symmetric, with a shape that closely resembles a normal distribution.\nMost importantly, we can observe that zero is not at the center of these distributions, especially for larger sample sizes. Instead, the center shifts toward the true average difference in donation rates of 0.004. This means that as we collect more data, the observed average is increasingly likely to reflect the true effect of treatment, and zero (no difference) lies in the tail.\nThis aligns with the real-world experiment: the treatment had a small but real effect on donation rates. The simulation confirms that if there were truly no effect, the sampling distribution would center on zero. But in our case, it clearly doesn’t.\n\n\nConclusion\nThis analysis replicates key results from Karlan and List (2007), showing that matching donation offers increase both the likelihood and total amount of charitable giving, though not necessarily the size of individual gifts once a donor decides to give. Through a combination of statistical modeling, visualization, and simulation, we confirmed the paper’s main findings and built intuition for why they hold, even when effect sizes are small. The evidence suggests that simple framing changes, like offering a match, can act as effective behavioral nudges, especially when applied at scale. From both a theoretical and practical perspective, this case demonstrates the power of combining randomized experimentation with statistical reasoning to inform smarter decision-making in marketing and fundraising."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\nsummary(blueprinty)\n\n    patents          region               age          iscustomer    \n Min.   : 0.000   Length:1500        Min.   : 9.00   Min.   :0.0000  \n 1st Qu.: 2.000   Class :character   1st Qu.:21.00   1st Qu.:0.0000  \n Median : 3.000   Mode  :character   Median :26.00   Median :0.0000  \n Mean   : 3.685                      Mean   :26.36   Mean   :0.3207  \n 3rd Qu.: 5.000                      3rd Qu.:31.62   3rd Qu.:1.0000  \n Max.   :16.000                      Max.   :49.00   Max.   :1.0000  \n\n\n\n\n\nTo explore potential differences in patent outcomes between Blueprinty customers and non-customers, we compare the distribution and average number of patents awarded across the two groups. This helps us understand whether Blueprinty users appear more successful in obtaining patents.\n\n# Histogram of number of patents by customer status\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 20) +\n  labs(title = \"Number of Patents by Customer Status\",\n       x = \"Number of Patents\",\n       fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Compare mean number of patents by customer status\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents, na.rm = TRUE),\n            sd_patents = sd(patents, na.rm = TRUE),\n            count = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents count\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n\nThe histogram reveals that both Blueprinty customers and non-customers exhibit a right-skewed distribution in the number of patents awarded over the past five years, with most firms receiving fewer than five patents. However, customers tend to have slightly higher counts overall. This visual impression is confirmed by the summary statistics: the average number of patents for Blueprinty customers is approximately 4.13, compared to 3.47 for non-customers. This suggests that firms using Blueprinty software may be somewhat more successful in obtaining patents.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution by customer status\nggplot(blueprinty, aes(x = as.factor(region), fill = as.factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Region Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion\",\n       fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Compare average age by customer status\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_age = mean(age, na.rm = TRUE),\n            sd_age = sd(age, na.rm = TRUE),\n            count = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_age sd_age count\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1          0     26.1   6.95  1019\n2          1     26.9   7.81   481\n\n\n\n# Histogram of age by customer status\nggplot(blueprinty, aes(x = age, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6) +\n  labs(title = \"Age Distribution by Customer Status\",\n       x = \"Firm Age (Years)\",\n       fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nExamining the regional breakdown, we find that Blueprinty customers are not evenly distributed across regions. In particular, a significantly higher proportion of customers are located in the Northeast, while other regions have relatively fewer customers. This suggests that customer status may be geographically clustered.\nIn terms of firm age, both customers and non-customers have similar distributions, with most firms falling between 15 and 35 years old. The average age of customer firms (26.9 years) is slightly higher than that of non-customers (26.1 years), but the difference is minimal.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents awarded to each firm, \\(Y_i\\), follows a Poisson distribution with rate parameter \\(\\lambda\\). The probability mass function is:\n\\[\nP(Y_i = y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nAssuming observations are independent, the likelihood function across all \\(n\\) firms is:\n\\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nTaking the natural logarithm gives the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + y_i \\log \\lambda - \\log y_i! \\right)\n\\]\nUsing the log-likelihood expression derived above, we now implement a simple R function that computes the log-likelihood for a Poisson model, given a value of λ and the observed data Y. This function will allow us to visualize and later optimize the likelihood.\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf) \n  sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\nTo better understand the shape of the Poisson likelihood function and where it reaches its maximum, we plot the log-likelihood across a range of plausible values of \\(\\lambda\\). This visual will give us a sense of the value of \\(\\lambda\\) that best fits the observed number of patents across firms.\n\nY &lt;- blueprinty$patents\n\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\nloglik_vals &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, Y))\n\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\")\n\n\n\n\n\n\n\n\nTo further build intuition for our Poisson model, we can derive the Maximum Likelihood Estimator analytically. By taking the first derivative of the log-likelihood with respect to \\(\\lambda\\), setting it to zero, and solving, we’ll see that the optimal value of \\(\\lambda\\) is simply the average of the observed data — a result that aligns with our understanding of the Poisson distribution.\nWe begin with the log-likelihood: \\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + y_i \\log \\lambda - \\log y_i! \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n y_i\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n y_i = 0\n\\Rightarrow \\lambda = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}\n\\]\nThus, the MLE for \\(\\lambda\\) is the sample mean of \\(Y\\).\nTo confirm our analytical result, we now use numerical optimization to estimate \\(\\lambda\\) by maximizing the log-likelihood function. We use the optim() function to find the value that best fits our observed patent count data.\n\nmle_result &lt;- optim(par = 1,\n                    fn = function(lambda) -poisson_loglikelihood(lambda, blueprinty$patents),\n                    method = \"Brent\",\n                    lower = 0.01, upper = 20)\n\nmle_result$par\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now generalize our Poisson model to allow for firm-specific rates of patent success by introducing covariates. In this regression setting, each firm’s expected number of patents \\(\\lambda_i\\) depends on its characteristics through the exponential function:\n\\[\n\\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis ensures the rate stays positive and allows us to estimate the effect of predictors like age, region, and customer status.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                     \n  lambda &lt;- exp(eta)                  \n  loglik &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))  \n  return(-loglik)  \n}\n\nTo estimate the Poisson regression model, we use our custom log-likelihood function together with optim() to find the MLE for \\(\\beta\\). We include a constant, age, age squared, region dummies (excluding one), and a customer indicator as covariates. We also extract the Hessian matrix to compute standard errors for each coefficient.\n\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(age2 = age^2,\n         region = as.factor(region),\n         iscustomer = as.numeric(iscustomer))\n\nX &lt;- model.matrix(~ age + age2 + region + iscustomer, data = blueprinty)\nY &lt;- blueprinty$patents\n\ninit_beta &lt;- rep(0, ncol(X))\n\nfit &lt;- optim(par = init_beta,\n             fn = poisson_regression_loglikelihood,\n             Y = Y,\n             X = X,\n             hessian = TRUE,\n             method = \"BFGS\")\n\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nse &lt;- sqrt(diag(solve(hessian)))\n\nresults &lt;- tibble(term = colnames(X),\n                  estimate = beta_hat,\n                  std_error = se)\n\nresults\n\n# A tibble: 8 × 3\n  term            estimate std_error\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112    \n2 age              0.116   0.00636  \n3 age2            -0.00223 0.0000771\n4 regionNortheast -0.0246  0.0434   \n5 regionNorthwest -0.0348  0.0529   \n6 regionSouth     -0.00544 0.0524   \n7 regionSouthwest -0.0378  0.0472   \n8 iscustomer       0.0607  0.0321   \n\n\nTo confirm our custom MLE implementation, we fit the same Poisson regression model using built-in glm() function. This allows us to compare coefficient estimates and standard errors.\n\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n               data = blueprinty,\n               family = poisson())\n\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe notice that the results from glm() and our custom optim() implementation are not identical. While both aim to estimate the same Poisson regression model, differences arise due to several practical factors.\nFirst, the two approaches construct model matrices differently. The glm() function automatically handles factor encoding, reference levels, and formula parsing. In contrast, optim() relied on a manually constructed matrix using model.matrix(), which may lead to different baseline categories or variable scaling — especially for categorical variables like region.\nSecond, the optimization methods differ. glm() uses a tailored algorithm (IRLS) that is fast and stable for Poisson models. optim(), on the other hand, is a general-purpose optimizer that may be more sensitive to the choice of starting values or the curvature of the likelihood surface.\nFinally, glm() incorporates automatic scaling and starting point heuristics that make it more robust for real-world data, while optim() starts from a default of all zeros and may not reach the global maximum without careful tuning.\nOverall, glm() is the preferred method for estimating Poisson regression models due to its stability and convenience, while the custom optim() approach is valuable for learning the mechanics of maximum likelihood estimation.\nWhile the iscustomer coefficient is positive and significant, interpreting the effect size directly from the log-link model is not straightforward. Instead, we simulate the expected number of patents under two scenarios for each firm — once assuming they are all customers, and once assuming they are not. The difference gives us an interpretable average treatment effect.\n\nX_0 &lt;- blueprinty %&gt;%\n  mutate(iscustomer = 0)\n\nX_1 &lt;- blueprinty %&gt;%\n  mutate(iscustomer = 1)\n\n# Predict using the glm model\ny_pred_0 &lt;- predict(glm_fit, newdata = X_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_fit, newdata = X_1, type = \"response\")\n\n# Compute average difference in predicted patent counts\navg_effect &lt;- mean(y_pred_1 - y_pred_0)\n\navg_effect\n\n[1] 0.7927681"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\nsummary(blueprinty)\n\n    patents          region               age          iscustomer    \n Min.   : 0.000   Length:1500        Min.   : 9.00   Min.   :0.0000  \n 1st Qu.: 2.000   Class :character   1st Qu.:21.00   1st Qu.:0.0000  \n Median : 3.000   Mode  :character   Median :26.00   Median :0.0000  \n Mean   : 3.685                      Mean   :26.36   Mean   :0.3207  \n 3rd Qu.: 5.000                      3rd Qu.:31.62   3rd Qu.:1.0000  \n Max.   :16.000                      Max.   :49.00   Max.   :1.0000  \n\n\n\n\n\nTo explore potential differences in patent outcomes between Blueprinty customers and non-customers, we compare the distribution and average number of patents awarded across the two groups. This helps us understand whether Blueprinty users appear more successful in obtaining patents.\n\n# Histogram of number of patents by customer status\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 20) +\n  labs(title = \"Number of Patents by Customer Status\",\n       x = \"Number of Patents\",\n       fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Compare mean number of patents by customer status\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents, na.rm = TRUE),\n            sd_patents = sd(patents, na.rm = TRUE),\n            count = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents count\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n\nThe histogram reveals that both Blueprinty customers and non-customers exhibit a right-skewed distribution in the number of patents awarded over the past five years, with most firms receiving fewer than five patents. However, customers tend to have slightly higher counts overall. This visual impression is confirmed by the summary statistics: the average number of patents for Blueprinty customers is approximately 4.13, compared to 3.47 for non-customers. This suggests that firms using Blueprinty software may be somewhat more successful in obtaining patents.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution by customer status\nggplot(blueprinty, aes(x = as.factor(region), fill = as.factor(iscustomer))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Region Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Proportion\",\n       fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Compare average age by customer status\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(mean_age = mean(age, na.rm = TRUE),\n            sd_age = sd(age, na.rm = TRUE),\n            count = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_age sd_age count\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1          0     26.1   6.95  1019\n2          1     26.9   7.81   481\n\n\n\n# Histogram of age by customer status\nggplot(blueprinty, aes(x = age, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6) +\n  labs(title = \"Age Distribution by Customer Status\",\n       x = \"Firm Age (Years)\",\n       fill = \"Customer\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nExamining the regional breakdown, we find that Blueprinty customers are not evenly distributed across regions. In particular, a significantly higher proportion of customers are located in the Northeast, while other regions have relatively fewer customers. This suggests that customer status may be geographically clustered.\nIn terms of firm age, both customers and non-customers have similar distributions, with most firms falling between 15 and 35 years old. The average age of customer firms (26.9 years) is slightly higher than that of non-customers (26.1 years), but the difference is minimal.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents awarded to each firm, \\(Y_i\\), follows a Poisson distribution with rate parameter \\(\\lambda\\). The probability mass function is:\n\\[\nP(Y_i = y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nAssuming observations are independent, the likelihood function across all \\(n\\) firms is:\n\\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nTaking the natural logarithm gives the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + y_i \\log \\lambda - \\log y_i! \\right)\n\\]\nUsing the log-likelihood expression derived above, we now implement a simple R function that computes the log-likelihood for a Poisson model, given a value of λ and the observed data Y. This function will allow us to visualize and later optimize the likelihood.\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf) \n  sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\nTo better understand the shape of the Poisson likelihood function and where it reaches its maximum, we plot the log-likelihood across a range of plausible values of \\(\\lambda\\). This visual will give us a sense of the value of \\(\\lambda\\) that best fits the observed number of patents across firms.\n\nY &lt;- blueprinty$patents\n\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\nloglik_vals &lt;- sapply(lambda_vals, function(l) poisson_loglikelihood(l, Y))\n\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     main = \"Log-Likelihood of Poisson Model\",\n     xlab = expression(lambda),\n     ylab = \"Log-Likelihood\")\n\n\n\n\n\n\n\n\nTo further build intuition for our Poisson model, we can derive the Maximum Likelihood Estimator analytically. By taking the first derivative of the log-likelihood with respect to \\(\\lambda\\), setting it to zero, and solving, we’ll see that the optimal value of \\(\\lambda\\) is simply the average of the observed data — a result that aligns with our understanding of the Poisson distribution.\nWe begin with the log-likelihood: \\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + y_i \\log \\lambda - \\log y_i! \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n y_i\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n y_i = 0\n\\Rightarrow \\lambda = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}\n\\]\nThus, the MLE for \\(\\lambda\\) is the sample mean of \\(Y\\).\nTo confirm our analytical result, we now use numerical optimization to estimate \\(\\lambda\\) by maximizing the log-likelihood function. We use the optim() function to find the value that best fits our observed patent count data.\n\nmle_result &lt;- optim(par = 1,\n                    fn = function(lambda) -poisson_loglikelihood(lambda, blueprinty$patents),\n                    method = \"Brent\",\n                    lower = 0.01, upper = 20)\n\nmle_result$par\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now generalize our Poisson model to allow for firm-specific rates of patent success by introducing covariates. In this regression setting, each firm’s expected number of patents \\(\\lambda_i\\) depends on its characteristics through the exponential function:\n\\[\n\\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis ensures the rate stays positive and allows us to estimate the effect of predictors like age, region, and customer status.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta                     \n  lambda &lt;- exp(eta)                  \n  loglik &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))  \n  return(-loglik)  \n}\n\nTo estimate the Poisson regression model, we use our custom log-likelihood function together with optim() to find the MLE for \\(\\beta\\). We include a constant, age, age squared, region dummies (excluding one), and a customer indicator as covariates. We also extract the Hessian matrix to compute standard errors for each coefficient.\n\nblueprinty &lt;- blueprinty %&gt;%\n  mutate(age2 = age^2,\n         region = as.factor(region),\n         iscustomer = as.numeric(iscustomer))\n\nX &lt;- model.matrix(~ age + age2 + region + iscustomer, data = blueprinty)\nY &lt;- blueprinty$patents\n\ninit_beta &lt;- rep(0, ncol(X))\n\nfit &lt;- optim(par = init_beta,\n             fn = poisson_regression_loglikelihood,\n             Y = Y,\n             X = X,\n             hessian = TRUE,\n             method = \"BFGS\")\n\nbeta_hat &lt;- fit$par\nhessian &lt;- fit$hessian\nse &lt;- sqrt(diag(solve(hessian)))\n\nresults &lt;- tibble(term = colnames(X),\n                  estimate = beta_hat,\n                  std_error = se)\n\nresults\n\n# A tibble: 8 × 3\n  term            estimate std_error\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -0.126   0.112    \n2 age              0.116   0.00636  \n3 age2            -0.00223 0.0000771\n4 regionNortheast -0.0246  0.0434   \n5 regionNorthwest -0.0348  0.0529   \n6 regionSouth     -0.00544 0.0524   \n7 regionSouthwest -0.0378  0.0472   \n8 iscustomer       0.0607  0.0321   \n\n\nTo confirm our custom MLE implementation, we fit the same Poisson regression model using built-in glm() function. This allows us to compare coefficient estimates and standard errors.\n\nglm_fit &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n               data = blueprinty,\n               family = poisson())\n\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe notice that the results from glm() and our custom optim() implementation are not identical. While both aim to estimate the same Poisson regression model, differences arise due to several practical factors.\nFirst, the two approaches construct model matrices differently. The glm() function automatically handles factor encoding, reference levels, and formula parsing. In contrast, optim() relied on a manually constructed matrix using model.matrix(), which may lead to different baseline categories or variable scaling — especially for categorical variables like region.\nSecond, the optimization methods differ. glm() uses a tailored algorithm (IRLS) that is fast and stable for Poisson models. optim(), on the other hand, is a general-purpose optimizer that may be more sensitive to the choice of starting values or the curvature of the likelihood surface.\nFinally, glm() incorporates automatic scaling and starting point heuristics that make it more robust for real-world data, while optim() starts from a default of all zeros and may not reach the global maximum without careful tuning.\nOverall, glm() is the preferred method for estimating Poisson regression models due to its stability and convenience, while the custom optim() approach is valuable for learning the mechanics of maximum likelihood estimation.\nWhile the iscustomer coefficient is positive and significant, interpreting the effect size directly from the log-link model is not straightforward. Instead, we simulate the expected number of patents under two scenarios for each firm — once assuming they are all customers, and once assuming they are not. The difference gives us an interpretable average treatment effect.\n\nX_0 &lt;- blueprinty %&gt;%\n  mutate(iscustomer = 0)\n\nX_1 &lt;- blueprinty %&gt;%\n  mutate(iscustomer = 1)\n\n# Predict using the glm model\ny_pred_0 &lt;- predict(glm_fit, newdata = X_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_fit, newdata = X_1, type = \"response\")\n\n# Compute average difference in predicted patent counts\navg_effect &lt;- mean(y_pred_1 - y_pred_0)\n\navg_effect\n\n[1] 0.7927681"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nIn this section, we use the number of reviews as a proxy for the number of bookings. We begin by cleaning the data and exploring basic relationships, then fit a Poisson regression model to understand how listing features influence review count, and by extension, booking activity.\nWe begin by loading the dataset and inspecting its structure.\n\n\n\n\n\n\nDataset\n\n\n\n\n\n\nlibrary(tidyverse)\n\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nglimpse(airbnb)\n\nRows: 40,628\nColumns: 14\n$ ...1                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ id                        &lt;dbl&gt; 2515, 2595, 3647, 3831, 4611, 5099, 5107, 51…\n$ days                      &lt;dbl&gt; 3130, 3127, 3050, 3038, 3012, 2981, 2981, 29…\n$ last_scraped              &lt;chr&gt; \"4/2/2017\", \"4/2/2017\", \"4/2/2017\", \"4/2/201…\n$ host_since                &lt;chr&gt; \"9/6/2008\", \"9/9/2008\", \"11/25/2008\", \"12/7/…\n$ room_type                 &lt;chr&gt; \"Private room\", \"Entire home/apt\", \"Private …\n$ bathrooms                 &lt;dbl&gt; 1, 1, 1, 1, NA, 1, 1, NA, 1, 1, 1, 1, 1, NA,…\n$ bedrooms                  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ price                     &lt;dbl&gt; 59, 230, 150, 89, 39, 212, 250, 60, 129, 79,…\n$ number_of_reviews         &lt;dbl&gt; 150, 20, 0, 116, 93, 60, 60, 50, 53, 329, 11…\n$ review_scores_cleanliness &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 8, 9, 7, 10, 9, 9, 9,…\n$ review_scores_location    &lt;dbl&gt; 9, 10, NA, 9, 8, 9, 9, 9, 10, 10, 10, 9, 10,…\n$ review_scores_value       &lt;dbl&gt; 9, 9, NA, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9…\n$ instant_bookable          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FAL…\n\n\n\n\n\nTo simplify the analysis, we remove rows with missing values in key variables and retain only those needed for modeling.\n\nairbnb_clean &lt;- airbnb %&gt;%\n  select(number_of_reviews, days, room_type, bathrooms, bedrooms, price,\n         review_scores_cleanliness, review_scores_location, review_scores_value,\n         instant_bookable) %&gt;%\n  drop_na()\n\nNext, we summarize the cleaned dataset to understand the distribution of key variables.\n\n\n\n\n\n\nSummary of the cleaned dataset\n\n\n\n\n\n\nsummary(airbnb_clean)\n\n number_of_reviews      days        room_type           bathrooms    \n Min.   :  1.00    Min.   :    7   Length:30160       Min.   :0.000  \n 1st Qu.:  3.00    1st Qu.:  584   Class :character   1st Qu.:1.000  \n Median :  8.00    Median : 1041   Mode  :character   Median :1.000  \n Mean   : 21.17    Mean   : 1140                      Mean   :1.122  \n 3rd Qu.: 26.00    3rd Qu.: 1592                      3rd Qu.:1.000  \n Max.   :421.00    Max.   :42828                      Max.   :6.000  \n    bedrooms          price         review_scores_cleanliness\n Min.   : 0.000   Min.   :   10.0   Min.   : 2.000           \n 1st Qu.: 1.000   1st Qu.:   70.0   1st Qu.: 9.000           \n Median : 1.000   Median :  103.0   Median :10.000           \n Mean   : 1.151   Mean   :  140.2   Mean   : 9.202           \n 3rd Qu.: 1.000   3rd Qu.:  169.0   3rd Qu.:10.000           \n Max.   :10.000   Max.   :10000.0   Max.   :10.000           \n review_scores_location review_scores_value instant_bookable\n Min.   : 2.000         Min.   : 2.000      Mode :logical   \n 1st Qu.: 9.000         1st Qu.: 9.000      FALSE:24243     \n Median :10.000         Median :10.000      TRUE :5917      \n Mean   : 9.415         Mean   : 9.334                      \n 3rd Qu.:10.000         3rd Qu.:10.000                      \n Max.   :10.000         Max.   :10.000                      \n\n\n\n\n\nAs a quick check, we visualize how the number of reviews varies with price. We log-transform price to reduce skew.\n\nggplot(airbnb_clean, aes(x = price, y = number_of_reviews)) +\n  geom_point(alpha = 0.3) +\n  scale_x_log10() +\n  labs(title = \"Number of Reviews vs. Price\", x = \"Price (log scale)\", y = \"Number of Reviews\")\n\n\n\n\n\n\n\n\nThe scatterplot reveals that listings with moderate prices tend to receive more reviews, while both very low-priced and high-priced listings receive fewer. Most listings are concentrated in the middle of the price range, where review counts are highest. On the log-scaled x-axis, we also see that listings above $500 are relatively rare and tend to have low review activity. This suggests that mid-priced listings may be more attractive to typical Airbnb users.\nBefore modeling, we convert categorical variables into factors so they are treated correctly in the regression.\n\nairbnb_clean &lt;- airbnb_clean %&gt;%\n  mutate(\n    room_type = as.factor(room_type),\n    instant_bookable = as.factor(instant_bookable)\n  )\n\nWe now fit a Poisson regression model with number of reviews as the outcome, and listing features as predictors.\n\nairbnb_model &lt;- glm(number_of_reviews ~ log(price + 1) + log(days + 1) +\n                      room_type + bathrooms + bedrooms +\n                      review_scores_cleanliness + review_scores_location + review_scores_value +\n                      instant_bookable,\n                    data = airbnb_clean,\n                    family = poisson())\n\nFinally, we inspect the model summary to interpret which factors influence review counts."
  }
]